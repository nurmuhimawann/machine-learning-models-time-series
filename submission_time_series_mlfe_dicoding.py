# -*- coding: utf-8 -*-
"""Submission_Time_Series_MLFE Dicoding.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fx0QM7jVHg82J4UMCX2481wkFAHYwOrk

# **Profile** <br>
*Nama: Nur Muhammad Himawan* <br>
*Domisili: Banyuwangi, Jawa Timur* <br>
*E-mail: muhammad.himawan73@gmail.com* <br>
*Path: Machine Learning & Front End Development* <br>
*Progam: Studi Independen Batch 3 - Kampus Merdeka*

## **Proyek Kedua : Membuat Model Machine Learning dengan Data Time Series**

Selamat, Anda telah menyelesaikan modul Time Series. Anda sudah mempelajari bagaimana mengembangkan jaringan saraf tiruan untuk masalah Time Series. Untuk bisa melanjutkan ke modul selanjutnya, Anda harus mengirimkan submission berupa proyek membuat model untuk prediksi Time Series.
"""

# Mount drive
from google.colab import drive
drive.mount('/content/drive')

"""**Dataset Preparation**"""

# API my kaggle
! mkdir ~/.kaggle
! cp '/content/drive/MyDrive/Colab Notebooks/Kaggle API/kaggle.json' ~/.kaggle/kaggle.json
! chmod 600 ~/.kaggle/kaggle.json
! ls ~/.kaggle

# source dataset -> https://www.kaggle.com/datasets/mahirkukreja/delhi-weather-data
! kaggle datasets download mahirkukreja/delhi-weather-data

# unzip
! unzip delhi-weather-data.zip -d /content/data/

"""**Load Dataset**

Delhi Weather Data.
This dataset contains weather data for New Delhi, India. It contains various features such as temperature, pressure, humidity, rain, precipitation,etc.
"""

# read_csv
import pandas as pd
df = pd.read_csv('/content/data/testset.csv')
df

# data info
df.info()

# change string object to datetime in columns 'dt'
df['datetime_utc'] = pd.to_datetime(df['datetime_utc'])
df.dtypes

"""**Selection Data**"""

# selection used columns (date and temp)
df = df[['datetime_utc',' _tempm' ]]
df

"""**Handling Missing Values**"""

# check missing value
df.isnull().sum()

# drop missing value
df.dropna(axis=0, inplace=True)

# check missing value again
df.isnull().sum()

# check data shape
df.shape

# description statistical about the dataset
df.describe()

"""**Split Train & Test Data**"""

# change value to numpy array
dates = df['datetime_utc'].values
temp = df[' _tempm'].values

# view date
dates

# view temperature
temp

# split train_test data
from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(temp, dates, test_size = 0.2)

print('amount of training data: {0}'.format(len(x_train)))
print('amount of validation data: {0}'.format(len(x_test)))

# check dtype
df.dtypes

"""**myCallbacks**"""

# 10% mae
limit = (df[' _tempm'].max() - df[' _tempm'].min()) * (10/100)
print(limit)

# callbacks
import tensorflow as tf
class myCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get('mae')<limit and logs.get('val_mae')<limit):
      print("\nMAE has reached below 10% of data scale, Stop Training!!")
      self.model.stop_training = True

callbacks = myCallback()

"""**Windowed dataset**"""

# accepts a series/attribute, and returns the labels and attributes of the dataset in batch form.
def windowed_dataset(series, window_size, batch_size, shuffle_buffer):
    series = tf.expand_dims(series, axis=-1)
    ds = tf.data.Dataset.from_tensor_slices(series)
    ds = ds.window(window_size + 1, shift=1, drop_remainder=True)
    ds = ds.flat_map(lambda w: w.batch(window_size + 1))
    ds = ds.shuffle(shuffle_buffer)
    ds = ds.map(lambda w: (w[:-1], w[-1:]))
    return ds.batch(batch_size).prefetch(1)

# train & val data
tf.keras.backend.set_floatx('float64')
training_set = windowed_dataset(x_train, window_size=60, batch_size=100, shuffle_buffer=1000)
validation_set = windowed_dataset(x_test, window_size=60, batch_size=100, shuffle_buffer=1000)

"""**Sequential Models**"""

# sequential models
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM
from tensorflow.keras.layers import Bidirectional
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import Dropout

model = tf.keras.models.Sequential([Bidirectional(LSTM(60, return_sequences=True)),
                                    Bidirectional(LSTM(60)),
                                    Dropout(0.5),
                                    Dense(30, activation="relu"),
                                    Dense(10, activation="relu"),
                                    Dense(1)])

# compile
optimizer = tf.keras.optimizers.SGD(learning_rate=1.0000e-04, momentum=0.9)

model.compile(loss=tf.keras.losses.Huber(),
              optimizer=optimizer,
              metrics=["mae"])

# fitting history
history = model.fit(training_set, 
                    epochs=25,
                    validation_data = validation_set,
                    callbacks=[callbacks])

"""**Evaluation Models**"""

import matplotlib.pyplot as plt

# Plot Accuracy
plt.plot(history.history['mae'])
plt.plot(history.history['val_mae'])
plt.title('MAE Model')
plt.ylabel('MAE')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper right')
plt.show()

# Plot Loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Loss Model')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper right')
plt.show()



"""## **Kriteria Penilaian**

Berikut kriteria submission yang harus Anda penuhi:

- Dataset yang akan dipakai bebas, namun minimal memiliki 1000 sampel. **(done)**

- Harus menggunakan LSTM dalam arsitektur model. **(done)**

- Validation set sebesar 20% dari total dataset. **(done)**

- Model harus menggunakan model sequential. **(done)**

- Harus menggunakan Learning Rate pada Optimizer. **(done)**

- MAE < 10% skala data. **(done)**

Anda dapat menerapkan beberapa saran untuk mendapatkan nilai tinggi, berikut sarannya:

- Dataset yang digunakan memiliki banyak sampel data. **(done)**

- Mengimplementasikan Callback. **(done)**

- Membuat plot loss dan akurasi pada saat training dan validation. **(done)**

Detail penilaian submission:

- Bintang 1 : Semua ketentuan terpenuhi, namun terdapat indikasi plagiat yaitu dengan menggunakan proyek orang lain dan hanya mengubah kontennya saja.

- Bintang 2 : Semua ketentuan terpenuhi, namun penulisan kode berantakan.

- Bintang 3 : Semua ketentuan terpenuhi namun hanya mengikuti seperti apa yang ada pada modul.

- Bintang 4 : Semua ketentuan terpenuhi, dataset memiliki minimal 2000 sampel data dan MAE dari model < 10% skala data.

- Bintang 5 : Semua ketentuan terpenuhi, dataset memiliki minimal 10000 sampel data dan MAE dari model < 10% skala data. **(done, I used 100317 samples of all the data in the dataset and MAE < 10%)**

## **Ketentuan Berkas Submission**

Beberapa poin yang perlu diperhatikan ketika mengirimkan berkas submission:

- Menggunakan bahasa pemrograman Python. **(done)**

- Mengirimkan pekerjaan Anda dalam bentuk berkas ipynb dan py dalam 1 folder yang telah di zip. **(done)**

- File ipynb yang dikirim telah dijalankan terlebih dahulu sehingga output telah ada tanpa reviewer perlu menjalankan ulang notebook. **(done)**
"""